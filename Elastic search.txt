# -*- coding: utf-8 -*-
"""CE706 Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dO1Lin3C0bfOA8_DfUm297msquS9dh_U
"""

!pip install elasticsearch

from elasticsearch import Elasticsearch

# Connect to Elasticsearch
es = Elasticsearch(
    hosts=["http://localhost:9200"]
)

# Defining the index name
index_name = 'signal_media_index'

# Defining the mapping for the index
mapping = {
    'mappings': {
        'properties': {
            'content': {
                'type': 'text',
                'fields': {
                    'raw': {
                        'type': 'keyword'
                    },
                    'tokenized': {
                        'type': 'text',
                        'analyzer': 'content_analyzer'
                    },
                    'ngrams': {
                        'type': 'text',
                        'analyzer': 'ngrams_analyzer'
                    },
                    'stemmed': {
                        'type': 'text',
                        'analyzer': 'stemming_analyzer'
                    }
                }
            }
        }
    }
}

# Creating the index with the specified mapping
es.indices.create(index=index_name, body=mapping)

# Defining the analyzers
tokenization_analyzer = {
    'tokenizer': 'standard',
    'filter': ['lowercase']
}

stopword_removal_analyzer = {
    'tokenizer': 'standard',
    'filter': ['lowercase', 'stop']
}

ngrams_analyzer = {
    'tokenizer': 'standard',
    'filter': ['lowercase', 'ngrams']
}

stemming_analyzer = {
    'tokenizer': 'standard',
    'filter': ['lowercase', 'stemmer']
}

# Defining the filter for the ngrams analyzer
ngrams_filter = {
    'type': 'ngram',
    'min_gram': 2,
    'max_gram': 3
}

# Defining the filter for the stemming analyzer
stemming_filter = {
    'type': 'stemmer',
    'name': 'english'
}

# Creating the custom analyzers
es.indices.close(index=index_name)

content_analyzer = {
    'tokenizer': 'standard',
    'filter': ['lowercase', 'stop', 'ngrams', 'stemmer']
}

es.indices.put_settings(index=index_name, body={
    'analysis': {
        'analyzer': {
            'content_analyzer': content_analyzer
        },
        'filter': {
            'ngrams': ngrams_filter,
            'stemmer': stemming_filter
        }
    }
})

es.indices.open(index=index_name)

import json
# identifying the file path
file_path = '/content/drive/MyDrive/sample-1M.jsonl'

# Reading the file line by line and parse each line as JSON

with open(file_path, 'r') as f:
    for line in f:
        data = json.loads(line)

data

# Defining the function to preprocess the content field
def preprocess_content(text):
    # Tokenizing the text
    tokens = es.indices.analyze(body={'text': text, 'analyzer': 'content_analyzer'})['tokens']
    # Removing stopwords
    tokens = [token['token'] for token in tokens if token['type'] != 'stop']
    # Joining the tokens back into text
    preprocessed_text = ' '.join(tokens)
    return preprocessed_text

# Preprocess the content field
preprocessed_content = preprocess_content(data['content'])
     
      # Index the preprocessed document
es.index(index=index_name, id=data['id'], body={
            'content': data['content'],
            'content_preprocessed': preprocessed_content
             })

# Defining the TF-IDF scripted similarity
tf_script = {
    'source': """
    double tf = Math.log(doc.freq + 1);
    double idf = Math.log((params.num_docs + 1) / (params.doc_freq + 1)) + 1;
    return tf * idf;
    """
}

tfidf_similarity = {
    'scripted_weight': {
        'params': {
            'num_docs': 0,
            'doc_freq': 0
        },
        'script': tf_script
    }
}

# Setting the TF-IDF similarity as the default similarity for the index
es.indices.put_settings(index=index_name, body={
    'similarity': {
        'tfidf_similarity': tfidf_similarity
    },
    'index': {
        'default_similarity': 'tfidf_similarity'
    }
})

# Checking output of the Analyzer
text = "The quick brown fox jumps over the lazy dog."

# Analyze the text using the content_analyzer
tokens = es.indices.analyze(body={'text': text, 'analyzer': 'content_analyzer'})['tokens']

# Print the tokens
print(tokens)

[  {'token': 'quick', 'start_offset': 4, 'end_offset': 9, 'type': '<ALPHANUM>'}, 
 {'token': 'brown', 'start_offset': 10, 'end_offset': 15, 'type': '<ALPHANUM>'},  
 {'token': 'fox', 'start_offset': 16, 'end_offset': 19, 'type': '<ALPHANUM>'},  
 {'token': 'jump', 'start_offset': 20, 'end_offset': 25, 'type': '<ALPHANUM>'}, 
 {'token': 'over', 'start_offset': 26, 'end_offset': 30, 'type': '<ALPHANUM>'},  
 {'token': 'lazi', 'start_offset': 35, 'end_offset': 39, 'type': '<ALPHANUM>'},  
 {'token': 'dog', 'start_offset': 40, 'end_offset': 43, 'type': '<ALPHANUM>'}]

"""TASK 2"""

import spacy

# Load the SpaCy English model
nlp = spacy.load('en_core_web_sm')

# Define a function to extract named entities from a given text
def extract_named_entities(text):
    doc = nlp(text)
    named_entities = []
    for entity in doc.ents:
        named_entities.append(entity.text)
    return '; '.join(named_entities)

# Looping through all documents in the data index
for doc in data['hits']['hits']:
    # Get the content field of the document
    content = doc['_source']['content']
    # Extract named entities from the content field
    named_entities = extract_named_entities(content)
    # Add the named entities to a new field called content_ner
    doc['_source']['content_ner'] = named_entities

# Defining a new analyzer for the content_ner field
es.indices.create(index='signal_media', body={
    'settings': {
        'analysis': {
            'analyzer': {
                'content_ner_analyzer': {
                    'tokenizer': 'my_tokenizer',
                    'filter': ['lowercase']
                }
            },
            'tokenizer': {
                'my_tokenizer': {
                    'type': 'pattern',
                    'pattern': '; '
                }
            }
        }
    },
    'mappings': {
        'properties': {
            'content_ner': {
                'type': 'text',
                'analyzer': 'content_ner_analyzer'
            }
        }
    }
})

# Example Search for documents containing the named entity "New York"
query = {
    'query': {
        'match': {
            'content_ner': 'New York'
        }
    }
}
result = es.search(index='signal_media', body=query)

"""TASK 3"""

# process the first 1000 documents with the analyzers
for i, doc in enumerate(data[:1000]):
    content = doc["content"]
    named_entities = nlp(content).ents
    named_entities_str = "; ".join(set([ne.text for ne in named_entities]))
    es.index(index="signal_media_index", id=i+1, body={"content": content, "content_ner": named_entities_str})

"""TSK 4"""

#Query: "climate change"
query = {
    "query": {
        "multi_match": {
            "query": "climate change",
            "fields": ["content", "content_ner^2"],
            "type": "most_fields"
        }
    }
}

res = es.search(index="signal_media_index", body=query, size=10)

#Query: "Apple Inc."
{
    "query": {
        "match": {
            "content_ner": "Apple Inc."
        }
    }
}

#Query: "United States election"
{
    "query": {
        "multi_match": {
            "query": "United States election",
            "fields": ["content", "content_ner^2"],
            "type": "most_fields"
        }
    }
}
